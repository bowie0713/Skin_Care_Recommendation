---
title: "skincare"
author: "Christina Pham"
date: "2025-01-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(httr)
library(rvest)
library(jsonlite)
library(tidyverse)
library(reticulate)
library(dplyr)
library(stringr)
```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

sitemap_url = "https://www.sephora.com/sitemaps/products-sitemap_en-CA.xml"
```

```{python}
from selenium import webdriver
import time

# Initialize Selenium WebDriver
#options = webdriver.ChromeOptions()
#options.add_argument("--disable-gpu")
#driver = webdriver.Chrome(options = options)  # Ensure you have ChromeDriver installed
#driver.get(sitemap_url)

#time.sleep(5)

# Get the page source after fully loading
#sitemap_html = driver.page_source

#driver.quit()

#Load html file
with open("sitemap.html", "r", encoding="utf-8") as file:
    sitemap_html = file.read()
```

```{r}
#save sitemap
#sitemap_html <- py$sitemap_html
#writeLines(sitemap_html, "sitemap.html")

# TO DO: 
#Check for unavailable prods 
# <h1 class="css-tz2kl4 e15t7owz0" data-comp="BaseComponent ">Sorry, this product is not available.</h1>

```

```{python}
soup = BeautifulSoup(sitemap_html, 'html.parser')
#soup
rows = soup.select('loc')
#rows
rows = pd.DataFrame(rows)

```

```{r}
# rows_html %>% 
#   filter(
#     str_detect(link, "shampoo") | str_detect(link, "conditioner") | str_detect(link, "cleanser") | str_detect(link, "cleanse") | str_detect(0, "cleansing") | str_detect(link, "cream") | str_detect(link, "creme") | str_detect(link, "moisturizer") | str_detect(0, "toner") | str_detect(link, "serum") | str_detect(link, "mask") | str_detect(link, "essence")
#   )
```


```{r}
rows_html <- py$rows
colnames(rows_html) <- c("link")

categories <- list(
  shampoo = c('shampoo'), 
  conditioner = c('conditioner'),
  cleanser = c('cleanser', 'cleanse', 'cleansing'),
  cream = c('cream','creme','moisturizer'),
  toner = c('toner'),
  serum = c('serum', 'essence'),
  mask = c('mask')
)
categorized_links <- list()

for (category in names(categories)) {
  keywords <- categories[[category]]
    filtered_links <- rows_html %>% 
    filter(
      sapply(keywords, function(keyword) str_detect(link, keyword)) %>% 
        rowSums() > 0
    )
    categorized_links[[category]] <- filtered_links
}

combined_df <- do.call(rbind, lapply(names(categorized_links), function(category) {
  data <- categorized_links[[category]]
  data$category <- category
  return(data)
}))

# Pass the combined DataFrame to Python
py$categorized_df <- combined_df

```

```{python}
import pandas as pd
print(categorized_df)
```

```{python}
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Load a subset of categorized_df for testing
test_links = categorized_df['link'][1]  # Use full dataset later

# Set up Selenium WebDriver with additional fixes
chrome_options = Options()
chrome_options.add_argument("--headless")  # Run in headless mode to avoid pop-ups
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--incognito")  # Forces a fresh session to prevent cache
chrome_options.add_argument("--disable-cache")  # Disable caching
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")  # Use realistic User-Agent

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=chrome_options)
driver.set_page_load_timeout(30)  # Allow up to 30 seconds for pages to load


driver = webdriver.Chrome(options=options)

def check_url_selenium(url, delay=10):
    """Check if a URL loads successfully using Selenium with a delay."""
    try:
        driver.delete_all_cookies()  # Clear cookies before each request
        driver.get(url)
        driver.refresh()  # Force a reload to prevent cache issues

        time.sleep(delay)  # Add a 10-second delay before checking validity

         # Wait for page load to complete before checking the URL
        WebDriverWait(driver, 10).until(EC.presence_of_element_located(("tag name", "body")))

        time.sleep(delay)  # Add a 10-second delay to allow JS to fully load
        driver.refresh()  # Force a reload to prevent cache issues

        # Get the final URL after possible redirections
        final_url = driver.current_url
        print(f"DEBUG: Final URL after first check - {final_url}")  # Debugging line
        
        # Wait another 10 seconds and check again in case of delayed redirects
        time.sleep(10)
        final_url = driver.current_url
        print(f"DEBUG: Final URL after waiting 2nd time - {final_url}")  # Debugging line
        
      
        # Check if the link was redirected to the 'product not carried' page
        if "productnotcarried" in final_url:
            return False  # If redirected to 'product not carried', mark as invalid
          
        return True  # If the page loads successfully and isn't redirected, mark as valid
      
    except WebDriverException:
        return False  # If it fails, mark as invalid

# List to store results
validity_results_selenium = []

# Iterate through all cleanser product URLs in the dataframe
for url in test_links:
    is_valid = check_url_selenium(url, delay = 10)
    validity_results_selenium.append((url, is_valid))

# Convert results into a pandas DataFrame
valid_cleanser_links_selenium = pd.DataFrame(validity_results_selenium, columns=["link", "valid"])

# Close Selenium WebDriver
driver.quit()

# Save the results to a CSV file
valid_cleanser_links_selenium.to_csv("valid_cleanser_links.csv", index=False)

# Display the final dataframe
print(valid_cleanser_links_selenium)

```

```{python}
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
product = pd.DataFrame()
product = pd.DataFrame(columns=['brandname', 'price'])

# Initialize Selenium WebDriver options
options = Options()
options.add_argument("--disable-gpu")

# Initialize the WebDriver
driver = webdriver.Chrome(options=options)  # Ensure ChromeDriver is installed and in PATH
# Loop through each link in categorized_df['link']
for link in categorized_df['link']:
  try:
    product = {}
    driver.get(link)
    time.sleep(10)
    try:
      #exit the pop up window
      xpath = '//*[@id="modalDialog"]/button'
      btn = driver.find_element_by_xpath(xpath)
      btn.click()
      time.sleep(10)
    except:
          pass
    content = driver.page_source       
    soup = BeautifulSoup(content, 'html.parser')
    product['brandname'] = soup.find_all('div', class_='css-1kj9pbo e15t7owz0').text()
    
      #product['productname'] = soup.find_all('span', class_='css-wkag1e e15t7owz0').text()
      #product['description'] = soup.find_all('div', class_='css-1kj9pbo e15t7owz0').text().strip()
    
    #products.append(product)
      # Do something with sitemap_html (e.g., save to a file or process it)
      
    # try: #check to see if the page is empty   
    #   if driver.find_element_by_class_name('css-3a7b4s').is_displayed():
    #     break
    #         
    #   except:
    
            #check to see if there is a pop up windew
            #as scrolling check if there is any more room to scroll
          # old_len = 0
          #   while True:
          #     browser = scrollDown(driver, 20) #scroll down the page
          #     time.sleep(10) #give it time to load
          #     slug = driver.find_elements_by_class_name('css-ix8km1') #look for the urls of products
          #     new_len = len(slug)
          #       if old_len == new_len: #if the old length and new length are equal, the bottom of page was reached
          #         break
          #       else:
          #         old_len = new_len

  except Exception as e:
    # Print any errors encountered
    print(f"Error processing {link}: {e}")
        

# Close the browser after the loop
driver.quit()

```


```{python}
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

#Testing
test_links = categorized_df['link'][:4]

# Initialize Selenium WebDriver options
# Setup Chrome options
options = Options()
options.add_argument("--disable-gpu")


# Initialize the WebDriver
driver = webdriver.Chrome(options=options)  # Ensure ChromeDriver is installed and in PATH

# Loop through each link in categorized_df['link']
products_list = []
for link in test_links:
    try:
        driver.get(link)
        time.sleep(15)  # Give page time to load
        
        #Check link if the page redirects to "productnotcarried"
        if "/search?" in driver.current_url:
          print(f" Skipping unavailable product: {link}")
          continue
        
        #Parse Page Source
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Extract brand name
        brand_element = soup.find_all('a', class_=['css-1kj9pbo e15t7owz0', 'css-wkag1e e15t7owz0'])
        brand_name = brand_element.text.strip() if brand_element else "N/A"

        # Append data
        products_list.append({"brandname": brand_name, "link": link})
        #check if it processes link 
        print(f" processed: {link}")

    except Exception as e:
        print(f" Error processing {link}: {e}")

# Close WebDriver *after* processing all links
driver.quit()

# Convert list to DataFrame
product_df = pd.DataFrame(products_list)
print(product_df)
View(product_df)
```

