---
title: "skincare"
author: "Christina Pham"
date: "2025-01-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(httr)
library(rvest)
library(jsonlite)
library(tidyverse)
library(reticulate)
library(dplyr)
library(stringr)
```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

sitemap_url = "https://www.sephora.com/sitemaps/products-sitemap_en-CA.xml"
```

```{python}
from selenium import webdriver
import time

# Initialize Selenium WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--disable-gpu")
driver = webdriver.Chrome(options = options)  # Ensure you have ChromeDriver installed
driver.get(sitemap_url)

time.sleep(5)

# Get the page source after fully loading
sitemap_html = driver.page_source

driver.quit()
```

```{r}
#save sitemap
sitemap_html <- py$sitemap_html
writeLines(sitemap_html, "sitemap.html")

# TO DO: 
#Check for unavailable prods 
# <h1 class="css-tz2kl4 e15t7owz0" data-comp="BaseComponent ">Sorry, this product is not available.</h1>

```

```{python}
soup = BeautifulSoup(sitemap_html, 'html.parser')
#soup
rows = soup.select('loc')
#rows
rows = pd.DataFrame(rows)
```

```{r}
# rows_html %>% 
#   filter(
#     str_detect(link, "shampoo") | str_detect(link, "conditioner") | str_detect(link, "cleanser") | str_detect(link, "cleanse") | str_detect(0, "cleansing") | str_detect(link, "cream") | str_detect(link, "creme") | str_detect(link, "moisturizer") | str_detect(0, "toner") | str_detect(link, "serum") | str_detect(link, "mask") | str_detect(link, "essence")
#   )
```

rows_html %>% 
  filter(
    str_detect(link, "shampoo") | str_detect(link, "conditioner") | str_detect(link, "cleanser") | str_detect(link, "cleanse") | str_detect(0, "cleansing") | str_detect(link, "cream") | str_detect(link, "creme") | str_detect(link, "moisturizer") | str_detect(0, "toner") | str_detect(link, "serum") | str_detect(link, "mask") | str_detect(link, "essence")
  )

```{r}
rows_html <- py$rows

colnames(rows_html) <- "link"

categories <- list(
  shampoo = c('shampoo'), 
  conditioner = c('conditioner'),
  cleanser = c('cleanser', 'cleanse', 'cleansing'),
  cream = c('cream','creme','moisturizer'),
  toner = c('toner'),
  serum = c('serum', 'essence'),
  mask = c('mask')
)
categorized_links <- list()

for (category in names(categories)) {
  keywords <- categories[[category]]
    filtered_links <- rows_html %>% 
    filter(
      sapply(keywords, function(keyword) str_detect(link, keyword)) %>% 
        rowSums() > 0
    )
    categorized_links[[category]] <- filtered_links
}

combined_df <- do.call(rbind, lapply(names(categorized_links), function(category) {
  data <- categorized_links[[category]]
  data$category <- category
  return(data)
}))

# Pass the combined DataFrame to Python
py$categorized_df <- combined_df
```

```{python}
import pandas as pd
print(categorized_df)
```

```{python}
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
product = pd.Dataframe()
product = pd.DataFrame(columns=['brandname', 'price'])

# Initialize Selenium WebDriver options
options = Options()
options.add_argument("--disable-gpu")

# Initialize the WebDriver
driver = webdriver.Chrome(options=options)  # Ensure ChromeDriver is installed and in PATH
# Loop through each link in categorized_df['link']
for link in categorized_df['link']:
  try:
    product = {}
    driver.get(link)
    time.sleep(10)
    try:
      #exit the pop up window
      xpath = '//*[@id="modalDialog"]/button'
      btn = driver.find_element_by_xpath(xpath)
      btn.click()
      time.sleep(10)
    except:
          pass
    content = driver.page_source       
    soup = BeautifulSoup(content, 'htlml.parser')
    product['brandname'] = soup.find_all('div', class_='css-1kj9pbo e15t7owz0').text()
    
      #product['productname'] = soup.find_all('span', class_='css-wkag1e e15t7owz0').text()
      #product['description'] = soup.find_all('div', class_='css-1kj9pbo e15t7owz0').text().strip()
    
    #products.append(product)
      # Do something with sitemap_html (e.g., save to a file or process it)
      
    # try: #check to see if the page is empty   
    #   if driver.find_element_by_class_name('css-3a7b4s').is_displayed():
    #     break
    #         
    #   except:
    
            #check to see if there is a pop up windew
            #as scrolling check if there is any more room to scroll
          # old_len = 0
          #   while True:
          #     browser = scrollDown(driver, 20) #scroll down the page
          #     time.sleep(10) #give it time to load
          #     slug = driver.find_elements_by_class_name('css-ix8km1') #look for the urls of products
          #     new_len = len(slug)
          #       if old_len == new_len: #if the old length and new length are equal, the bottom of page was reached
          #         break
          #       else:
          #         old_len = new_len

  except Exception as e:
    # Print any errors encountered
    print(f"Error processing {link}: {e}")
        

# Close the browser after the loop
driver.quit()

```

