---
title: "skincare"
author: "Christina Pham"
date: "2025-01-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(httr)
library(rvest)
library(jsonlite)
library(tidyverse)
library(reticulate)
library(dplyr)
library(stringr)
```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

sitemap_url = "https://www.sephora.com/sitemaps/products-sitemap_en-CA.xml"
```

```{python}
from selenium import webdriver
import time

# Initialize Selenium WebDriver
#options = webdriver.ChromeOptions()
#options.add_argument("--disable-gpu")
#driver = webdriver.Chrome(options = options)  # Ensure you have ChromeDriver installed
#driver.get(sitemap_url)

#time.sleep(5)

# Get the page source after fully loading
#sitemap_html = driver.page_source

#driver.quit()

#Load html file
with open("sitemap.html", "r", encoding="utf-8") as file:
    sitemap_html = file.read()
```

```{r}
#save sitemap
sitemap_html <- py$sitemap_html
writeLines(sitemap_html, "sitemap.html")
```

```{python}
soup = BeautifulSoup(sitemap_html, 'html.parser')
rows = soup.select('loc')
rows = pd.DataFrame(rows)

rows
```

```{r}
rows_html <- py$rows
colnames(rows_html) <- c("link")

categories <- list(
  shampoo = c('shampoo'), 
  conditioner = c('conditioner'),
  cleanser = c('cleanser', 'cleanse', 'cleansing'),
  cream = c('cream','creme','moisturizer'),
  toner = c('toner'),
  serum = c('serum', 'essence'),
  mask = c('mask')
)
categorized_links <- list()

for (category in names(categories)) {
  keywords <- categories[[category]]
    filtered_links <- rows_html %>% 
    filter(
      sapply(keywords, function(keyword) str_detect(link, keyword)) %>% 
        rowSums() > 0
    )
    categorized_links[[category]] <- filtered_links
}

combined_df <- do.call(rbind, lapply(names(categorized_links), function(category) {
  data <- categorized_links[[category]]
  data$category <- category
  return(data)
}))

# Pass the combined DataFrame to Python
py$categorized_df <- combined_df

```

```{python}
import pandas as pd
print(categorized_df)
```

<<<<<<< Updated upstream
=======

>>>>>>> Stashed changes
```{python}
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

#Testing
test_links = categorized_df['link'][:3]
print(categorized_df)

# Setup Chrome options
options = Options()
options.add_argument("--disable-gpu")
<<<<<<< Updated upstream

# MACS USE THIS: options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36")

options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36")
=======
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36") #Christina's agent 
>>>>>>> Stashed changes
options.add_argument("--disable-blink-features=AutomationControlled")

# Initialize the WebDriver
driver = webdriver.Chrome(options=options)  # Ensure ChromeDriver is installed and in PATH

# Loop through each link in categorized_df['link']
products_list = []
for link in test_links:
    try:
        driver.get(link)
        time.sleep(8)  # Give page time to load
        
        #Check link if the page redirects to "productnotcarried"
        if "/search?" in driver.current_url:
          print(f" Skipping unavailable product: {link}")
          continue
        
        #Parse Page Source
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Extract brand name
        brand_element = soup.find('a', class_=['css-1kj9pbo e15t7owz0', 'css-wkag1e e15t7owz0'])
        brand_name = brand_element.text.strip() if brand_element else "N/A"
        
        #Extract brand price
        brand_element3 = soup.find('b', class_='css-0')
        brand_price = brand_element3.text.strip() if brand_element3 else "N/A"
        
        #Extract brand rating
        brand_elements = soup.find_all('span', class_='css-egw4ri e15t7owz0')

        # Ensure there's at least one match before accessing index 0
        if brand_elements:
          brand_rating = brand_elements[0].text.strip()  # Get first match
        else:
          brand_rating = "N/A"  # Default value if no match found
        print(f"Brand Rating: {brand_rating}")
        
        # #Extract brand reviews
        # brand_element5 = soup.find('span', class_ = 'css-1dae9ku e15t7owz0')
        # brand_reviews = brand_element5.text.strip() if brand_element5 else "N/A"
        
        # Append data
        products_list.append({"Brand Name": brand_name, 
                              "Brand Price": brand_price,
                              "Brand Rating": brand_rating,
                          #    "Brand Reviews": brand_reviews,
                              "link": link})
        #check if it processes link 
        print(f" processed: {link}")

    except Exception as e:
        print(f" Error processing {link}: {e}")

# Close WebDriver *after* processing all links
driver.quit()

# Convert list to DataFrame
product_df = pd.DataFrame(products_list)
print(product_df)
View(product_df)
```

