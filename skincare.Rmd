---
title: "skincare"
author: "Christina Pham"
date: "2025-01-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(httr)
library(rvest)
library(jsonlite)
library(tidyverse)
library(reticulate)
library(dplyr)
library(stringr)
```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

sitemap_url = "https://www.sephora.com/sitemaps/products-sitemap_en-CA.xml"
```

```{python}
from selenium import webdriver
import time

# Initialize Selenium WebDriver
#options = webdriver.ChromeOptions()
#options.add_argument("--disable-gpu")
#driver = webdriver.Chrome(options = options)  # Ensure you have ChromeDriver installed
#driver.get(sitemap_url)

#time.sleep(5)

# Get the page source after fully loading
#sitemap_html = driver.page_source

#driver.quit()

#Load html file
with open("sitemap.html", "r", encoding="utf-8") as file:
    sitemap_html = file.read()
```

```{r}
#save sitemap
sitemap_html <- py$sitemap_html
writeLines(sitemap_html, "sitemap.html")
```

```{python}
soup = BeautifulSoup(sitemap_html, 'html.parser')
rows = soup.select('loc')
rows = pd.DataFrame(rows)

rows
```

```{r}
rows_html <- py$rows
colnames(rows_html) <- c("link")

categories <- list(
  shampoo = c('shampoo'), 
  conditioner = c('conditioner'),
  cleanser = c('cleanser', 'cleanse', 'cleansing'),
  cream = c('cream','creme','moisturizer'),
  toner = c('toner'),
  serum = c('serum', 'essence'),
  mask = c('mask')
)
categorized_links <- list()

for (category in names(categories)) {
  keywords <- categories[[category]]
    filtered_links <- rows_html %>% 
    filter(
      sapply(keywords, function(keyword) str_detect(link, keyword)) %>% 
        rowSums() > 0
    )
    categorized_links[[category]] <- filtered_links
}

combined_df <- do.call(rbind, lapply(names(categorized_links), function(category) {
  data <- categorized_links[[category]]
  data$category <- category
  return(data)
}))

# Pass the combined DataFrame to Python
py$categorized_df <- combined_df

```

```{python}
import pandas as pd
print(categorized_df)
```


```{python}
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

#Testing
test_links = categorized_df['URL'][:3]
# print(categorized_df)

def scrollDown(driver, n_scroll):
    elem = driver.find_element(By.TAG_NAME, "html")
    while n_scroll >= 0:
        elem.send_keys(Keys.PAGE_DOWN)
        n_scroll -= 1
    return driver

# Setup Chrome options
options = Options()
options.add_argument("--disable-gpu")

options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36")


options.add_argument("--disable-blink-features=AutomationControlled")

# Initialize the WebDriver
driver = webdriver.Chrome(options=options)  # Ensure ChromeDriver is installed and in PATH

# Loop through each link in categorized_df['link']
products_list = []
for link in test_links:
    try:
        driver.get(link)
        time.sleep(8)  # Give page time to load
        
        #Check link if the page redirects to "productnotcarried"
        if "/search?" in driver.current_url:
          print(f" Skipping unavailable product: {link}")
          continue
        
        while True:
            browser = scrollDown(driver, 20) #scroll down the page
            time.sleep(8) #give it time to load
            break
        
        #Parse Page Source
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Extract brand name
        brand_element = soup.find('a', class_=['css-1kj9pbo e15t7owz0', 'css-wkag1e e15t7owz0'])
        brand_name = brand_element.text.strip() if brand_element else "N/A"
        
      # TO DO: Take off size: 
        #Extract brand size
        size = soup.find('span', class_ = 'css-15ro776')
        product_size = size.text.strip() if size else "N/A"
        
        #Extract product price
        brand_element3 = soup.find('b', class_='css-0')
        prod_price = brand_element3.text.strip() if brand_element3 else "N/A"
        
        #Extract product rating class="css-egw4ri e15t7owz0"
        brand_elements = soup.find_all('span', class_= 'css-egw4ri e15t7owz0')

        # Ensure there's at least one match before accessing index 0
        if brand_elements:
          prod_rating = brand_elements[0].text.strip()  # Get first match
        else:
          prod_rating = "N/A"  # Default value if no match found
        print(f"Product Rating: {prod_rating}")
        
        # #Extract brand reviews
        # brand_element5 = soup.find('span', class_ = 'css-1dae9ku e15t7owz0')
        # prod_reviews = brand_element5.text.strip() if brand_element5 else "N/A"
        
        # Append data
        products_list.append({"Brand Name": brand_name, 
                              "Product Price": prod_price,
                              "Product Rating": prod_rating,
                              "Product Size": product_size,
                          #    "Product Reviews": prod_reviews,
                              "link": link})
        #check if it processes link 
        print(f" processed: {link}")

    except Exception as e:
        print(f" Error processing {link}: {e}")

# Close WebDriver *after* processing all links
driver.quit()

# Convert list to DataFrame
product_df = pd.DataFrame(products_list)
print(product_df)
View(product_df)
```

## Test
```{python}
```